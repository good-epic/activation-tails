{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_experiment_results(results_dir: str | Path):\n",
    "    \"\"\"\n",
    "    Load all results from an experiment directory.\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Path to the experiment results directory\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing all loaded results and statistics\n",
    "    \"\"\"\n",
    "    results_dir = Path(results_dir)\n",
    "    results = {}\n",
    "    \n",
    "    # Load arguments used for the run\n",
    "    with open(results_dir / \"args.json\", 'r') as f:\n",
    "        results['args'] = json.load(f)\n",
    "    \n",
    "    # Load data split information if it exists\n",
    "    split_file = results_dir / \"data_split.json\"\n",
    "    if split_file.exists():\n",
    "        with open(split_file, 'r') as f:\n",
    "            results['data_split'] = json.load(f)\n",
    "    \n",
    "    # Dictionary to store per-model results\n",
    "    results['models'] = {}\n",
    "    \n",
    "    # Load results for each model\n",
    "    for model_dir in results_dir.iterdir():\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        model_results = {}\n",
    "        \n",
    "        # Load main test results\n",
    "        test_results_file = model_dir / \"test_results.csv\"\n",
    "        if test_results_file.exists():\n",
    "            model_results['test_results'] = pd.read_csv(test_results_file)\n",
    "        \n",
    "        # Load per-layer likelihoods\n",
    "        layer_ll_file = model_dir / \"per_layer_likelihoods.csv\"\n",
    "        if layer_ll_file.exists():\n",
    "            model_results['layer_likelihoods'] = pd.read_csv(layer_ll_file)\n",
    "        \n",
    "        # Load matrices\n",
    "        matrices = {}\n",
    "        for matrix_name in ['confusion', 'precision', 'recall', 'f1']:\n",
    "            matrix_file = model_dir / f\"full_tuple_{matrix_name}_matrix.csv\"\n",
    "            if matrix_file.exists():\n",
    "                matrices[f'{matrix_name}_matrix'] = pd.read_csv(matrix_file, index_col=0)\n",
    "        model_results['matrices'] = matrices\n",
    "        \n",
    "        # Load top-k accuracy\n",
    "        topk_file = model_dir / \"full_tuple_top_k_accuracy.csv\"\n",
    "        if topk_file.exists():\n",
    "            model_results['top_k_accuracy'] = pd.read_csv(topk_file, index_col=0)\n",
    "        \n",
    "        # Load per-dimension results\n",
    "        dimension_results = {}\n",
    "        for dim_dir in model_dir.iterdir():\n",
    "            if not dim_dir.is_dir() or not dim_dir.name.startswith('dimension_'):\n",
    "                continue\n",
    "                \n",
    "            dim_matrices = {}\n",
    "            for matrix_name in ['confusion', 'precision', 'recall', 'f1']:\n",
    "                matrix_file = dim_dir / f\"{matrix_name}_matrix.csv\"\n",
    "                if matrix_file.exists():\n",
    "                    dim_matrices[f'{matrix_name}_matrix'] = pd.read_csv(matrix_file, index_col=0)\n",
    "            \n",
    "            topk_file = dim_dir / \"top_k_accuracy.csv\"\n",
    "            if topk_file.exists():\n",
    "                dim_matrices['top_k_accuracy'] = pd.read_csv(topk_file, index_col=0)\n",
    "                \n",
    "            dimension_results[dim_dir.name] = dim_matrices\n",
    "        \n",
    "        model_results['dimension_results'] = dimension_results\n",
    "        \n",
    "        # Add all model results to main results dict\n",
    "        results['models'][model_dir.name] = model_results\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_from_confusion(conf_mat: pd.DataFrame) -> tuple[pd.Series, pd.Series, pd.Series]:\n",
    "    \"\"\"Calculate precision, recall and F1 scores from a confusion matrix.\n",
    "    \n",
    "    Args:\n",
    "        conf_mat: Square pandas DataFrame with identical row and column labels,\n",
    "                 where rows are true labels and columns are predicted labels\n",
    "                 \n",
    "    Returns:\n",
    "        Tuple of (precision, recall, f1) where each is a pandas Series\n",
    "        with one value per category\n",
    "    \"\"\"\n",
    "    # Ensure matrix is square with matching labels\n",
    "    if not all(conf_mat.index == conf_mat.columns):\n",
    "        raise ValueError(\"Confusion matrix must have identical row and column labels\")\n",
    "        \n",
    "    # Calculate metrics for each category\n",
    "    precision = pd.Series(index=conf_mat.columns, dtype=float)\n",
    "    recall = pd.Series(index=conf_mat.index, dtype=float)\n",
    "    f1 = pd.Series(index=conf_mat.index, dtype=float)\n",
    "    \n",
    "    for category in conf_mat.columns:\n",
    "        # True positives are diagonal elements\n",
    "        tp = conf_mat.loc[category, category]\n",
    "        \n",
    "        # False positives are sum of column minus true positives\n",
    "        fp = conf_mat[category].sum() - tp\n",
    "        \n",
    "        # False negatives are sum of row minus true positives  \n",
    "        fn = conf_mat.loc[category].sum() - tp\n",
    "        \n",
    "        # Calculate metrics, handling division by zero\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        \n",
    "        # Calculate F1 as harmonic mean of precision and recall\n",
    "        f1_score = 2 * (prec * rec) / (prec + rec) if (prec + rec) > 0 else 0.0\n",
    "        \n",
    "        precision[category] = prec\n",
    "        recall[category] = rec\n",
    "        f1[category] = f1_score\n",
    "        \n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subcats(label: str) -> tuple[str, str]:\n",
    "    \"\"\"Split a category label into its two subcategories.\n",
    "    \n",
    "    Args:\n",
    "        label: String of format '(\"subcat1_i\", \"subcat2_i\")'\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (subcat1_i, subcat2_i)\n",
    "    \"\"\"\n",
    "    # Remove outer single quote, parentheses and split on comma\n",
    "    label = label.strip(\"'\").strip('()')\n",
    "    subcat1, subcat2 = label.split(',')\n",
    "    # Clean up quotes and whitespace\n",
    "    subcat1 = subcat1.strip().strip('\"')\n",
    "    subcat1 = subcat1.strip().strip(\"'\")\n",
    "    subcat2 = subcat2.strip().strip('\"')\n",
    "    subcat2 = subcat2.strip().strip(\"'\")\n",
    "    return (subcat1, subcat2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results_dir: str,\n",
    "                 figures_dir: str = None,\n",
    "                 show_plots: bool = False,\n",
    "                 model_names: list[str] = ['gpt2-small', 'gpt2-medium', 'gpt2-large', \n",
    "                                           'pythia-160m', 'pythia-410m', 'pythia-1b']):\n",
    "    \"\"\"\n",
    "    Plot and optionally save analysis results.\n",
    "    \n",
    "    Args:\n",
    "        results_dir: Directory containing results\n",
    "        figures_dir: Directory to save figures (defaults to results_dir/figs/)\n",
    "        show_plots: Whether to display plots in notebook\n",
    "        model_names: List of model names to include in analysis\n",
    "    \"\"\"\n",
    "    # Set up figures directory\n",
    "    if figures_dir is None:\n",
    "        figures_dir = os.path.join(results_dir, 'figs')\n",
    "    os.makedirs(figures_dir, exist_ok=True)\n",
    "\n",
    "    results = load_experiment_results(results_dir)\n",
    "\n",
    "    model_count = len(model_names)\n",
    "    # Get the row/column labels. They are all assumed the same\n",
    "    subcats1 = list(results['models'][model_names[0]]['test_results']['true_category_1'].unique())\n",
    "    subcats2 = list(results['models'][model_names[0]]['test_results']['true_category_2'].unique())\n",
    "\n",
    "    # Initialize DataFrames with correct columns\n",
    "    precision1_matrix = pd.DataFrame(index=model_names, columns=subcats1, dtype=float)\n",
    "    precision2_matrix = pd.DataFrame(index=model_names, columns=subcats2, dtype=float)\n",
    "    recall1_matrix = pd.DataFrame(index=model_names, columns=subcats1, dtype=float)\n",
    "    recall2_matrix = pd.DataFrame(index=model_names, columns=subcats2, dtype=float)\n",
    "    f1_1_matrix = pd.DataFrame(index=model_names, columns=subcats1, dtype=float)\n",
    "    f1_2_matrix = pd.DataFrame(index=model_names, columns=subcats2, dtype=float)\n",
    "\n",
    "    cmap_val = 'YlOrRd'\n",
    "    for model_nm in model_names:\n",
    "        # Column sums histogram\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        conf_mat = results['models'][model_nm]['matrices']['confusion_matrix']\n",
    "        col_sums = conf_mat.sum(axis=0)\n",
    "        plt.hist(col_sums, bins=50)\n",
    "        plt.title(f'Distribution of Column Sums - {model_nm}')\n",
    "        plt.xlabel('Sum')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig(os.path.join(figures_dir, f'{model_nm}_column_sums.png'), bbox_inches='tight')\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Confusion matrix heatmap\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.heatmap(conf_mat, annot=False, cmap=cmap_val, xticklabels=False, yticklabels=False)\n",
    "        plt.title(f'Confusion Matrix - {model_nm}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(figures_dir, f'{model_nm}_confusion_matrix.png'), bbox_inches='tight')\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Get labels from the confusion matrix indices and verify they match columns\n",
    "        labels = conf_mat.index\n",
    "        assert all(labels == conf_mat.columns), \"Row and column labels don't match\"\n",
    "\n",
    "        # Get the row/column labels\n",
    "        subcats1 = list(results['models'][model_nm]['test_results']['true_category_1'].unique())\n",
    "        subcats2 = list(results['models'][model_nm]['test_results']['true_category_2'].unique())\n",
    "\n",
    "        # Create empty matrices for each subcat\n",
    "        subcat1_matrix = pd.DataFrame(np.zeros((len(subcats1), len(subcats1))),\n",
    "                                    index=subcats1, columns=subcats1)\n",
    "        subcat2_matrix = pd.DataFrame(np.zeros((len(subcats2), len(subcats2))),\n",
    "                                    index=subcats2, columns=subcats2)\n",
    "\n",
    "        # Fill matrices by summing appropriate blocks\n",
    "        for i, label_i in enumerate(labels):\n",
    "            sub1_i, sub2_i = get_subcats(label_i)\n",
    "            for j, label_j in enumerate(labels):\n",
    "                sub1_j, sub2_j = get_subcats(label_j)\n",
    "                subcat1_matrix.loc[sub1_i, sub1_j] += conf_mat.loc[label_i, label_j]\n",
    "                subcat2_matrix.loc[sub2_i, sub2_j] += conf_mat.loc[label_i, label_j]\n",
    "\n",
    "        # Plot subcategory matrices\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.heatmap(subcat1_matrix, annot=True, fmt='.0f', xticklabels=subcats1, \n",
    "                   yticklabels=subcats1, cmap=cmap_val)\n",
    "        plt.title(f'Confusion Matrix for First Subcategory - {model_nm}')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.heatmap(subcat2_matrix, annot=True, fmt='.0f', xticklabels=subcats2, \n",
    "                   yticklabels=subcats2, cmap=cmap_val)\n",
    "        plt.title(f'Confusion Matrix for Second Subcategory - {model_nm}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(figures_dir, f'{model_nm}_subcategory_matrices.png'), bbox_inches='tight')\n",
    "        if show_plots:\n",
    "            plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        # Calculate metrics for subcategories\n",
    "        prec1, rec1, f1_1 = get_metrics_from_confusion(subcat1_matrix)\n",
    "        prec2, rec2, f1_2 = get_metrics_from_confusion(subcat2_matrix)\n",
    "\n",
    "        # Store in matrices\n",
    "        precision1_matrix.loc[model_nm] = prec1\n",
    "        precision2_matrix.loc[model_nm] = prec2\n",
    "        recall1_matrix.loc[model_nm] = rec1\n",
    "        recall2_matrix.loc[model_nm] = rec2\n",
    "        f1_1_matrix.loc[model_nm] = f1_1\n",
    "        f1_2_matrix.loc[model_nm] = f1_2\n",
    "\n",
    "    # Plot precision matrices\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(precision1_matrix, annot=True, fmt='.2f', cmap=cmap_val, vmin=0, vmax=1)\n",
    "    plt.title('Precision - First Subcategory')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(precision2_matrix, annot=True, fmt='.2f', cmap=cmap_val, vmin=0, vmax=1)\n",
    "    plt.title('Precision - Second Subcategory')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'precision_matrices.png'), bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot recall matrices\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(recall1_matrix, annot=True, fmt='.2f', cmap=cmap_val, vmin=0, vmax=1)\n",
    "    plt.title('Recall - First Subcategory')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(recall2_matrix, annot=True, fmt='.2f', cmap=cmap_val, vmin=0, vmax=1)\n",
    "    plt.title('Recall - Second Subcategory')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'recall_matrices.png'), bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot F1 matrices\n",
    "    plt.figure(figsize=(18, 8))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(f1_1_matrix, annot=True, fmt='.2f', cmap=cmap_val, vmin=0, vmax=1)\n",
    "    plt.title('F1 - First Subcategory')\n",
    "    plt.yticks(rotation=0)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(f1_2_matrix, annot=True, fmt='.2f', cmap=cmap_val, vmin=0, vmax=1)\n",
    "    plt.title('F1 - Second Subcategory')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(figures_dir, 'f1_matrices.png'), bbox_inches='tight')\n",
    "    if show_plots:\n",
    "        plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path_all_points = \"/home/mattylev/projects/transformers/distributions/kde_classification/results/20250107_4/\"\n",
    "results_path_tails = \"/home/mattylev/projects/transformers/distributions/kde_classification/results/20250109_1/\"\n",
    "results_path_tails_2 = \"/home/mattylev/projects/transformers/distributions/kde_classification/results/20250109_2/\"\n",
    "#results = load_experiment_results(results_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_path_tails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_path_all_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(results_path_tails_2, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We're getting the exact same confusion matrix, and thus derivative metrics, for tails only and all points analysis. Obviously\n",
    "## that can't be right. Both analyses are fully done at this point. So load the test results and see if you can get any hints about \n",
    "## WTF is going on?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('about celebrities', 'in all lower case')    38\n",
       "('about poetry', 'in all lower case')         41\n",
       "('about vacation', 'in all lower case')       41\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_mat.sum(axis=1)[conf_mat.sum(axis=1) != 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('about music', 'in a kind tone')                    664\n",
      "('about politics', 'in english')                     617\n",
      "('about poetry', 'in a concise style')               239\n",
      "('about politics', 'in a concise style')             235\n",
      "('about celebrities', 'in a concise style')          181\n",
      "('about celebrities', 'in all lower case')           178\n",
      "('about celebrities', 'in english')                  148\n",
      "('about music', 'in spanish')                        143\n",
      "('about poetry', 'in japanese')                      125\n",
      "('about computer science', 'in an angry tone')       105\n",
      "('about science fiction', 'in japanese')              96\n",
      "('about american football', 'in an angry tone')       76\n",
      "('about american football', 'in all lower case')      71\n",
      "('about celebrities', 'in japanese')                  69\n",
      "('about computer science', 'in a childish style')     68\n",
      "('about american football', 'in all upper case')      67\n",
      "('about poetry', 'in spanish')                        61\n",
      "('about effective altruism', 'in japanese')           61\n",
      "('about effective altruism', 'in all upper case')     57\n",
      "('about politics', 'in all upper case')               57\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "conf_mat = results['models']['pythia-410m']['matrices']['confusion_matrix']\n",
    "col_sums = conf_mat.sum(axis=0)\n",
    "sorted_sums = col_sums.sort_values(ascending=False)\n",
    "print(sorted_sums.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['matrices']['confusion_matrix'].loc[labels[0], labels[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['('about american football', 'in a childish style')',\n",
       "       '('about american football', 'in a concise style')',\n",
       "       '('about american football', 'in a didactic style')',\n",
       "       '('about american football', 'in a flowery style')',\n",
       "       '('about american football', 'in a helpful style')',\n",
       "       '('about american football', 'in a kind tone')',\n",
       "       '('about american football', 'in all lower case')',\n",
       "       '('about american football', 'in all upper case')',\n",
       "       '('about american football', 'in an angry tone')',\n",
       "       '('about american football', 'in english')',\n",
       "       ...\n",
       "       '('about vacation', 'in a didactic style')',\n",
       "       '('about vacation', 'in a flowery style')',\n",
       "       '('about vacation', 'in a helpful style')',\n",
       "       '('about vacation', 'in a kind tone')',\n",
       "       '('about vacation', 'in all lower case')',\n",
       "       '('about vacation', 'in all upper case')',\n",
       "       '('about vacation', 'in an angry tone')',\n",
       "       '('about vacation', 'in english')', '('about vacation', 'in japanese')',\n",
       "       '('about vacation', 'in spanish')'],\n",
       "      dtype='object', length=120)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results['matrices']['confusion_matrix'].index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transdemo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
